{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import datasets\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-03T17:48:18.224417Z",
     "end_time": "2023-07-03T17:48:18.233007Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-07-03T17:15:34.617039Z",
     "end_time": "2023-07-03T17:15:34.655600Z"
    }
   },
   "outputs": [],
   "source": [
    "# load video data\n",
    "video_data = datasets.Dataset.load_from_disk(\"../data/videos/\")\n",
    "descriptions = video_data['description']\n",
    "titles = video_data['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "\" Former President Donald Trump weighs in on allegations against the Biden family's business dealings on 'Hannity.'\\\\n\\\\nSubscribe to Fox News! https://bit.ly/2vBUvAS\\\\nWatch more Fox News Video: http://vi...\""
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean descriptions\n",
    "des = descriptions[7567]\n",
    "des"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-03T16:39:58.450531Z",
     "end_time": "2023-07-03T16:39:58.457100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[102], line 13\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m descriptions:\n\u001B[0;32m      5\u001B[0m     d \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m      6\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m(Subscribe to)|\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      7\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m((http).*(Watch))|\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     11\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mu0026 Friends Weekend)|\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     12\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mn)|(\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mr)\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, d)\n\u001B[1;32m---> 13\u001B[0m     \u001B[43mpre_processed_descriptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappend\u001B[49m(d)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m titles:\n\u001B[0;32m     15\u001B[0m     t \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m     16\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m(Subscribe to)|\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     17\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m((http).*(Watch))|\u001B[39m\u001B[38;5;124m'\u001B[39m  \u001B[38;5;66;03m# complete youtube links\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     22\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mn)|(\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mr)|\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     23\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m| The Stream)\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, t)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'dict' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# clean all descriptions\n",
    "pre_processed_descriptions = []\n",
    "pre_processed_titles = []\n",
    "for d in descriptions:\n",
    "    d = re.sub(\n",
    "        '(Subscribe to)|'\n",
    "        '((http).*(Watch))|'\n",
    "        '((http).*(\\.\\.\\.))|'\n",
    "        '(News Agency)|(News)|'\n",
    "        '(Fox)|(CNN)|(BBC)|(DW)|(CCTV)|(Al Jazeera)|(#foxnews)|(#fox)|(#ingrahamangle)'\n",
    "        '(\\\\\\\\u0026 Friends Weekend)|'\n",
    "        '(\\\\\\\\n)|(\\\\\\\\r)', '', d)\n",
    "    pre_processed_descriptions.append(d)\n",
    "for t in titles:\n",
    "    t = re.sub(\n",
    "        '(Subscribe to)|'\n",
    "        '((http).*(Watch))|'  # complete youtube links\n",
    "        '((http).*(\\.\\.\\.))|'  # cut off links\n",
    "        '(Newsfeed)|(News Agency)|(News)|'\n",
    "        '(Fox)|(CNN)|(BBC)|(DW)|(CCTV)|(Al Jazeera)|(#foxnews)|(#fox)|(#ingrahamangle)'  # cha\n",
    "        '(\\\\\\\\u0026 Friends Weekend)|'\n",
    "        '(\\\\\\\\n)|(\\\\\\\\r)|'\n",
    "        '(\\| The Stream)', '', t)\n",
    "    pre_processed_titles.append(t)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-03T17:22:15.687065Z",
     "end_time": "2023-07-03T17:22:16.762820Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/7908 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f7f73f6bcc24598b7ded88d814ac78f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/7908 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8aebcc0c8b93478da0d0c59dd7a32f06"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Provided `function` which is applied to all elements of table returns a variable of type <class 'list'>. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[101], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m dataset_with_length \u001B[38;5;241m=\u001B[39m video_data\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x: {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlength\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mlen\u001B[39m(x[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m\"\u001B[39m])})\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# add values to new column\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m tokenized_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mvideo_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpre_processed_titles\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\projects\\NLP-Clustering-News-Outlets\\venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:580\u001B[0m, in \u001B[0;36mtransmit_tasks.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    578\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    579\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 580\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    581\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    582\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[0;32m    583\u001B[0m     \u001B[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\projects\\NLP-Clustering-News-Outlets\\venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:545\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    538\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    539\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[0;32m    540\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[0;32m    541\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[0;32m    542\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[0;32m    543\u001B[0m }\n\u001B[0;32m    544\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 545\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    546\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    547\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\projects\\NLP-Clustering-News-Outlets\\venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3087\u001B[0m, in \u001B[0;36mDataset.map\u001B[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[0;32m   3079\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transformed_dataset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3080\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mtqdm(\n\u001B[0;32m   3081\u001B[0m         disable\u001B[38;5;241m=\u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mis_progress_bar_enabled(),\n\u001B[0;32m   3082\u001B[0m         unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m examples\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3085\u001B[0m         desc\u001B[38;5;241m=\u001B[39mdesc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMap\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3086\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[1;32m-> 3087\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m rank, done, content \u001B[38;5;129;01min\u001B[39;00m Dataset\u001B[38;5;241m.\u001B[39m_map_single(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdataset_kwargs):\n\u001B[0;32m   3088\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[0;32m   3089\u001B[0m                 shards_done \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\Documents\\projects\\NLP-Clustering-News-Outlets\\venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3463\u001B[0m, in \u001B[0;36mDataset._map_single\u001B[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001B[0m\n\u001B[0;32m   3459\u001B[0m indices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\n\u001B[0;32m   3460\u001B[0m     \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m*\u001B[39m(\u001B[38;5;28mslice\u001B[39m(i, i \u001B[38;5;241m+\u001B[39m batch_size)\u001B[38;5;241m.\u001B[39mindices(shard\u001B[38;5;241m.\u001B[39mnum_rows)))\n\u001B[0;32m   3461\u001B[0m )  \u001B[38;5;66;03m# Something simpler?\u001B[39;00m\n\u001B[0;32m   3462\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3463\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[43mapply_function_on_filtered_inputs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3464\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3465\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3466\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_same_num_examples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mshard\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlist_indexes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3467\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3468\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3469\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m NumExamplesMismatchError:\n\u001B[0;32m   3470\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DatasetTransformationNotAllowedError(\n\u001B[0;32m   3471\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3472\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\projects\\NLP-Clustering-News-Outlets\\venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3355\u001B[0m, in \u001B[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001B[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001B[0m\n\u001B[0;32m   3352\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m update_data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3353\u001B[0m     \u001B[38;5;66;03m# Check if the function returns updated examples\u001B[39;00m\n\u001B[0;32m   3354\u001B[0m     update_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28misinstance\u001B[39m(processed_inputs, (Mapping, pa\u001B[38;5;241m.\u001B[39mTable))\n\u001B[1;32m-> 3355\u001B[0m     \u001B[43mvalidate_function_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocessed_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3356\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m update_data:\n\u001B[0;32m   3357\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# Nothing to update, let's move on\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\projects\\NLP-Clustering-News-Outlets\\venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3300\u001B[0m, in \u001B[0;36mDataset._map_single.<locals>.validate_function_output\u001B[1;34m(processed_inputs, indices)\u001B[0m\n\u001B[0;32m   3298\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Validate output of the map function.\"\"\"\u001B[39;00m\n\u001B[0;32m   3299\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m processed_inputs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processed_inputs, (Mapping, pa\u001B[38;5;241m.\u001B[39mTable)):\n\u001B[1;32m-> 3300\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m   3301\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProvided `function` which is applied to all elements of table returns a variable of type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(processed_inputs)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3302\u001B[0m     )\n\u001B[0;32m   3303\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(indices, \u001B[38;5;28mlist\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processed_inputs, Mapping):\n\u001B[0;32m   3304\u001B[0m     allowed_batch_return_types \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mlist\u001B[39m, np\u001B[38;5;241m.\u001B[39mndarray, pd\u001B[38;5;241m.\u001B[39mSeries)\n",
      "\u001B[1;31mTypeError\u001B[0m: Provided `function` which is applied to all elements of table returns a variable of type <class 'list'>. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects."
     ]
    }
   ],
   "source": [
    "# TODO: assign values or create new dataset\n",
    "# Process the dataset - add a column with the length of the context texts\n",
    "dataset_with_length = video_data.map(lambda x: {\"length\": len(x[\"title\"])})\n",
    "# add values to new column\n",
    "tokenized_dataset = video_data.map(lambda x: pre_processed_titles, batched=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
