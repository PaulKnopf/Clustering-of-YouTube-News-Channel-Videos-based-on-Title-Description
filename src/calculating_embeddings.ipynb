{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data\n",
    "import datasets\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get device proper device to run on \n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else: \n",
    "    # calculating the embeddings on a cpu can take hours, not recommended\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting null values\n",
      "title                             1\n",
      "views                             0\n",
      "time-stamp-upload-milliseconds    0\n",
      "time-stamp                        0\n",
      "date-time-hr                      0\n",
      "upload-time-hr                    0\n",
      "upload-time-stamp                 0\n",
      "description                       0\n",
      "video-length                      0\n",
      "video-length-milliseconds         0\n",
      "channel                           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "all_records_df = load_data(\"../data/\")\n",
    "\n",
    "# find and delete null values\n",
    "print(\"counting null values\")\n",
    "print(all_records_df.isnull().sum())\n",
    "\n",
    "all_records_df.dropna(axis=0, inplace=True)\n",
    "\n",
    "# also we want a df of individual videos that are identified by title and description\n",
    "videos_df = all_records_df.drop_duplicates([\"title\", \"description\"], keep=\"last\")\n",
    "# note: if you want the first record in videos_df, enter vidoes_df.iloc[0] not videos_df[0]\n",
    "# as the later will try to return the record that has index or id = 0  wich is the index in the all_records_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_record_to_string(record):\n",
    "    \"\"\"\n",
    "    record should be Dict-like\n",
    "    record should contain keys \"title\" and \"description\"\n",
    "    this will return a single string containing title and description information\n",
    "    \"\"\"\n",
    "    title = record[\"title\"].strip()\n",
    "    description = record[\"description\"].strip()\n",
    "\n",
    "    out = \"Title: '{}' Description: '{}'\".format(title, description)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'id'],\n",
       "    num_rows: 7908\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn the dataframe into a datasets.Dataset with cols \"text\", \"id\"\n",
    "combined = videos_df[[\"title\", \"description\"]].apply(combine_record_to_string, axis=1)\n",
    "data = datasets.Dataset.from_pandas(combined.to_frame(\"text\"))\n",
    "data = data.rename_column(\"__index_level_0__\", \"id\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mxwur\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    encoded_input = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "    )\n",
    "    return encoded_input\n",
    "\n",
    "def tokenize_record(record):\n",
    "    return tokenize_text(record[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ff062a6f884309a58cdcaf7f1dff1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_tokenized = data.map(tokenize_record)\n",
    "data_tokenized.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(input):\n",
    "    with torch.no_grad():\n",
    "        model_output = model.encoder(\n",
    "            input_ids=input[\"input_ids\"].to(device),\n",
    "            attention_mask=input[\"attention_mask\"].to(device),\n",
    "        )\n",
    "        last_hidden_state = model_output.last_hidden_state\n",
    "\n",
    "    return dict(embeddings=last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f36fb88c8724e9b9e291cc555343870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_tokenized = data_tokenized.map(get_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47dbf692643a4b489c1603b75ebf2d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/25 shards):   0%|          | 0/7908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save to disk\n",
    "data_tokenized.select_columns([\"id\", \"embeddings\"]).save_to_disk(\"../data/embeddings\")\n",
    "# can be read with\n",
    "# datasets.Dataset.load_from_disk(\"../data/embeddings/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
